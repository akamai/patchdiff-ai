import asyncio
import datetime
import difflib
import json
import time
import uuid

from dataclasses import dataclass
from pathlib import Path

from binexport import FunctionBinExport
from langchain_core.documents import Document
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.messages.utils import count_tokens_approximately
from langchain_core.prompt_values import PromptValue
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph
from pydantic import BaseModel, Field

from agent import Agent
from agent_tools.vector_store import VectorStore
from common import (
    AgentModels,
    StateInfo,
    Artifact,
    logger,
    Timer,
    Report,
    CveDetails,
    LLM,
    Threshold,
    console,
)
from defaultdataclass import defaultdataclass, field
from vr_agent.prompts import SYSTEM_SCORE_FUNCTIONS, SYSTEM_GENERATE_REPORT


@defaultdataclass
class DecompiledFunctionMetadata:
    file: str
    name: str
    address: str
    parents: str


@defaultdataclass
class DecompiledFunction:
    before: str
    after: str
    udiff: str
    metadata: DecompiledFunctionMetadata
    score: float


@defaultdataclass
class VulnerabilityResearchContext:
    state_info: StateInfo
    artifact: Artifact
    cve_details: CveDetails
    decompiled: list[DecompiledFunction]
    reports: list[Report] = field(default_factory=list)


@defaultdataclass
class VulnerabilityResearchOutput:
    reports: list[Report]  # = field(default_factory=list)


@defaultdataclass
class PromptTemplate:
    analyze: ChatPromptTemplate
    rank: ChatPromptTemplate


class Metadata(BaseModel):
    file: str = Field(
        ..., description="Filename where the function resides (e.g., 'kernel32.dll')"
    )
    name: str = Field(..., description="Function name (e.g., 'NtCreateFile')")
    address: str = Field(
        ..., description="Function address or RVA (e.g., '0x7FFFB12A')"
    )


class FunctionRelevancy(BaseModel):
    metadata: Metadata = Field(
        ...,
        description="Dictionary containing exactly the keys 'file', 'name', and 'address'",
    )
    score: float = Field(
        ...,
        description="Scoring rubric (hard boundaries):\n"
        "0.00 … Clearly cosmetic / refactor\n"
        "0.25 … Unlikely security, minor logic tweak\n"
        "0.50 … Possibly related (some safety hints)\n"
        "0.75 … Probable security fix (strong indicators)\n"
        "1.00 … Direct, obvious patch for a vulnerability",
    )
    why: str = Field(..., description="Explain why you score this function like this")


class VulnFuncs(BaseModel):
    functions: list[FunctionRelevancy] = Field(
        ..., description="List of FunctionRelevancy objects."
    )

    class Config:
        json_schema_extra = {
            "example": [
                {
                    "metadata": {
                        "file": "kernel32.dll",
                        "name": "NtCreateFile",
                        "address": "7FFFB12A",
                    },
                    "score": 0.78,
                },
                {
                    "metadata": {
                        "file": "ntoskrnl.exe",
                        "name": "ExAllocatePoolWithTag",
                        "address": "FFFFF800",
                    },
                    "score": 0.45,
                },
            ]
        }


class VulnReport(BaseModel):
    found: bool = Field(..., description="If the vulnerability was found successfully")
    confidence: float = Field(
        ...,
        description="Confidence of the right vulnerability was found - score from 0.0 to 1.0 "
        "higher is better",
    )
    report: str = Field(..., description="The report")


def discover_parents(func: FunctionBinExport, iteration=0):
    if func:
        if not func.parents or iteration > 9:
            yield [func.name]
        else:
            for parent in func.parents:
                for prefix in discover_parents(parent, iteration + 1):
                    yield prefix + [func.name]
    else:
        yield None


async def add_to_store(artifact: Artifact):
    with Timer(f"indexing {artifact.primary_file.name} to vector store"):
        primary_path = Path(artifact.primary_file.path).parent / "__funcs__"
        secondary_path = Path(artifact.secondary_file.path).parent / "__funcs__"
        logger.info(
            f"{artifact.primary_file.name} has {len(artifact.changed)} functions modified "
            f"in {artifact.primary_file.kb} update"
        )

        new_artifacts: list[Document] = []
        primary_metadata = artifact.primary_file.to_dict()
        primary_metadata["version"] = ".".join(
            str(x) for x in primary_metadata["version"]
        )

        secondary_metadata = artifact.secondary_file.to_dict()
        secondary_metadata["version"] = ".".join(
            str(x) for x in secondary_metadata["version"]
        )

        for func in artifact.changed:
            before_code = None
            after_code = None

            res = VectorStore.func_logic.get(
                where={
                    "$and": [
                        {"uid": artifact.secondary_file.uid},
                        {"address": f"{func.address2:X}"},
                    ]
                },
            )
            if not res.get("ids"):
                try:
                    # TODO: maybe fallback to base
                    before_code = (secondary_path / f"{func.address2:X}.c").read_text(
                        encoding="utf-8"
                    )
                except FileNotFoundError:
                    continue

            res = VectorStore.func_logic.get(
                where={
                    "$and": [
                        {"uid": artifact.primary_file.uid},
                        {"address": f"{func.address1:X}"},
                    ]
                },
            )
            if not res.get("ids"):
                try:
                    after_code = (primary_path / f"{func.address1:X}.c").read_text(
                        encoding="utf-8"
                    )
                except FileNotFoundError:
                    continue

            if before_code and after_code:
                new_artifacts.append(
                    Document(
                        page_content=after_code,
                        metadata={
                            **primary_metadata,
                            "address": f"{func.address1:X}",
                            "parents": json.dumps(
                                next(
                                    discover_parents(
                                        artifact.diff.primary.get(func.address1)
                                    ),
                                    None,
                                )
                            ),
                        },
                    )
                )
                new_artifacts.append(
                    Document(
                        page_content=before_code,
                        metadata={
                            **secondary_metadata,
                            "address": f"{func.address2:X}",
                            "parents": json.dumps(
                                next(
                                    discover_parents(
                                        artifact.diff.secondary.get(func.address2)
                                    ),
                                    None,
                                )
                            ),
                        },
                    )
                )

        if new_artifacts:
            try:
                await VectorStore.func_logic.aadd_documents(
                    documents=new_artifacts,
                    ids=[str(uuid.uuid4()) for i in range(len(new_artifacts))],
                )

                logger.info(
                    f"{len(new_artifacts)} functions from {artifact.primary_file.name} "
                    f"were embedded to vector store"
                )
            except BaseException as e:
                logger.error(f"Failed to add functions to vector store: {e}")


class VulnerabilityResearch(Agent):
    """
    Agent that can analyze vulnerabilities within snippet code with context
    1. Get list of code snippet pairs (before and after) and rank relevancy by its probability to be a security change
    2. Get list of code snippet pairs (before and after) and find vulnerability according to context and generate report
    3. Human interaction using chat
    4. Windbg MCP (* Future)
    """

    prompt_template: PromptTemplate = PromptTemplate(
        rank=ChatPromptTemplate(
            [
                SystemMessage(SYSTEM_SCORE_FUNCTIONS),
                MessagesPlaceholder("functions"),
                MessagesPlaceholder("metadata"),
            ]
        ),
        analyze=ChatPromptTemplate(
            [
                SystemMessage(SYSTEM_GENERATE_REPORT),
                MessagesPlaceholder("functions"),
                MessagesPlaceholder("metadata"),
            ]
        ),
    )

    @dataclass(frozen=True)
    class NODES:
        indexing = "Indexing artifacts"
        rank = "Rank artifacts"
        analyze = "Root cause analysis"
        generate = "Generate report"

    def __init__(self):
        super().__init__()
        self._iter = 3

    def _build(self):
        if self._graph:
            return

        builder = StateGraph(
            state_schema=VulnerabilityResearchContext,
            output=VulnerabilityResearchOutput,
        )

        builder.add_node(self.NODES.analyze, self.analyze)
        builder.add_node(self.NODES.rank, self.rank)
        builder.add_node(self.NODES.generate, self.generate)
        builder.add_node(self.NODES.indexing, self.indexing)

        builder.set_entry_point(self.NODES.indexing)
        builder.add_edge(self.NODES.indexing, self.NODES.rank)
        builder.add_edge(self.NODES.rank, self.NODES.analyze)
        builder.add_conditional_edges(
            self.NODES.analyze, self.refinement, [self.NODES.rank, self.NODES.generate]
        )
        builder.set_finish_point(self.NODES.generate)

        self._graph = builder.compile()

    def refinement(self, context: VulnerabilityResearchContext, config: RunnableConfig):
        self._iter -= 1
        th = config.get("configurable", {}).get("threshold", Threshold())
        if context.reports and context.reports[0].confidence > th.report:
            console.info(
                f"[+] Found {context.cve_details.cve} security flaw "
                f"with confidence: {context.reports[0].confidence}"
            )
            return self.NODES.generate
        else:
            # Try again
            if self._iter > 0 and context.decompiled:
                console.info(
                    f"[-] Cannot find {context.cve_details.cve} distinct security flaw, try again"
                )
                return self.NODES.rank
            else:
                console.info(
                    f"[-] Failed to find {context.cve_details.cve} security flaw within the iteration limit"
                )
                return self.NODES.generate

    async def indexing(self, context: VulnerabilityResearchContext):
        context.state_info.node.append(self.NODES.indexing)
        # await add_to_store(context.artifact)

        primary_path = Path(context.artifact.primary_file.path).parent / "__funcs__"
        secondary_path = Path(context.artifact.secondary_file.path).parent / "__funcs__"
        logger.info(
            f"{context.artifact.primary_file.name} has {len(context.artifact.changed)} functions modified "
            f"in {context.artifact.primary_file.kb} update"
        )

        functions: list[DecompiledFunction] = []
        for func in context.artifact.changed:
            before_code = None
            after_code = None

            try:
                # TODO: maybe fallback to base
                before_code = (secondary_path / f"{func.address2:X}.c").read_text(
                    encoding="utf-8"
                )
            except FileNotFoundError:
                continue

            try:
                after_code = (primary_path / f"{func.address1:X}.c").read_text(
                    encoding="utf-8"
                )
            except FileNotFoundError:
                continue

            # before = VectorStore.func_logic.get(
            #     where={'$and': [{'uid': context.artifact.secondary_file.uid},
            #                     {'address': f'{func.address2:X}'}]},
            # )
            #
            # after = VectorStore.func_logic.get(
            #     where={'$and': [{'uid': context.artifact.primary_file.uid},
            #                     {'address': f'{func.address1:X}'}]},
            # )

            try:
                udiff = difflib.unified_diff(
                    # before['documents'][0].splitlines(),
                    # after['documents'][0].splitlines(),
                    before_code.splitlines(),
                    after_code.splitlines(),
                    fromfile=f"{func.name2} before",
                    tofile=f"{func.name1} after",
                    lineterm="",
                )
            except BaseException as e:
                continue

            functions.append(
                DecompiledFunction(
                    before=before_code,
                    after=after_code,
                    udiff="\n".join(udiff),
                    metadata=DecompiledFunctionMetadata(
                        file=context.artifact.primary_file.name,
                        name=func.name1,
                        address=f"{func.address1:X}",
                        # parents=after['metadatas'][0].get('parents'),
                        parents=json.dumps(
                            next(
                                discover_parents(
                                    context.artifact.diff.secondary.get(func.address1)
                                ),
                                None,
                            )
                        ),
                    ),
                )
            )

        return {
            "decompiled": functions,
        }

    async def rank(self, context: VulnerabilityResearchContext):
        context.state_info.node.append(self.NODES.rank)

        if not context.decompiled:
            logger.error("No decompiled functions")
            return {}

        changes = [
            HumanMessage(
                f"\n\n------------------------------------\n"
                f"function metadata: {func.metadata}\n"
                f"changes:\n{func.udiff}\n\n"
                f"------------------------------------\n\n"
            )
            for func in context.decompiled
            if func.score is None
        ]

        metadata = [
            HumanMessage(
                f"Metadata about the vulnerability: ["
                f"{context.cve_details.cve}"
                f"{context.cve_details.msrc_report.title}"
                f"{context.cve_details.msrc_report.description}"
                f"{context.cve_details.msrc_report.faq}"
                f"{context.cve_details.msrc_report.cwe}]"
            )
        ]

        prompt: PromptValue | None = None
        n = 0
        for i in range(len(changes), 0, -1):
            prompt = self.prompt_template.rank.invoke(
                {
                    "functions": changes[:i],
                    "metadata": metadata,
                }
            )

            if count_tokens_approximately(prompt) < 100_000:
                n = i
                break

        if not prompt:
            return {}

        console.info(
            f"[*] Ranking {n} modified artifacts out of {len(context.decompiled)}"
        )
        llm = AgentModels.reverse_engineering_model
        result = await llm.model.with_structured_output(
            VulnFuncs, include_raw=True
        ).ainvoke(prompt)

        if result["raw"].usage_metadata:
            logger.info(
                f'Usage: {result["raw"].usage_metadata.get("total_tokens")} tokens using {llm.name}'
            )

        if result["parsing_error"]:
            logger.error(f'parsing error: {result["parsing_error"]}')
            return {}

        scored_functions: list[FunctionRelevancy] = result["parsed"].functions
        logger.debug(
            "\n".join(str(x) for x in scored_functions)
        )  # TODO: change to debug

        for scored in scored_functions:
            for func in context.decompiled:
                if scored.metadata.address == func.metadata.address:
                    func.score = scored.score
                    break

        return {"decompiled": context.decompiled}

    async def analyze(
        self, context: VulnerabilityResearchContext, config: RunnableConfig
    ):
        context.state_info.node.append(self.NODES.analyze)

        if not context.decompiled:
            logger.error("No decompiled functions")
            return {}

        th = config.get("configurable", {}).get("threshold", Threshold())
        functions: list[DecompiledFunction] = []
        for func in context.decompiled:
            if not func.score or func.score < th.security_modification:
                continue
            functions.append(func)

        if len(functions):
            console.info(
                f"[+] Found {len(functions)} possible security modification "
                f"in {context.artifact.primary_file.name}"
            )
        else:
            console.info(
                f"[-] Failed to find security modification "
                f"in {context.artifact.primary_file.name}"
            )

        changes = [
            HumanMessage(
                f"\n\n------------------------------------\n"
                f"function metadata: {artifact.metadata}\n"
                f"before patch:\n{artifact.before}\n\n"
                f"patch:\n{artifact.udiff}\n\n"
                f"------------------------------------\n\n"
            )
            for artifact in functions
        ]

        metadata = [
            HumanMessage(
                f"Metadata about the vulnerability: ["
                f"{context.cve_details.cve}"
                f"{context.cve_details.msrc_report.title}"
                f"{context.cve_details.msrc_report.description}"
                f"{context.cve_details.msrc_report.faq}"
                f"{context.cve_details.msrc_report.cwe}]"
            )
        ]

        prompt: PromptValue | None = None
        count = 0
        for i in range(len(changes), 0, -1):
            prompt = self.prompt_template.analyze.invoke(
                {"functions": changes[:i], "metadata": metadata}
            )
            if count_tokens_approximately(prompt) < 100_000:
                count = i
                break

        if not prompt:
            return {}

        console.info(
            f"[*] Analyze {context.artifact.primary_file.name} using {count} artifacts"
        )

        if config.get("configurable", {}).get("evaluate", False):
            models = [LLM.claude_sonnet, LLM.claude_opus, LLM.o3, ]
            tasks = [self.analyze_with_model(prompt, model) for model in models]
            analysis_reports = await asyncio.gather(*tasks)
        else:
            analysis_reports = [
                await self.analyze_with_model(prompt, AgentModels.researcher_model)
            ]

        remainder = [f for f in context.decompiled if f not in functions]

        reports = []
        for report, model in analysis_reports:
            if report is None:
                continue
            if report.found:
                reports.append(
                    Report(
                        cve_details=context.cve_details,
                        artifact=context.artifact,
                        content=report.report,
                        confidence=report.confidence,
                        model=model
                    )
                )

        return {
            "reports": [*(context.reports or []), *reports],
            "decompiled": remainder,
        }

    async def analyze_with_model(self, prompt, llm):
        result = await llm.model.with_structured_output(
            VulnReport, include_raw=True
        ).ainvoke(prompt)

        if result["raw"].usage_metadata:
            logger.info(
                f'Usage: {result["raw"].usage_metadata.get("total_tokens")} tokens using {llm.name}'
            )

        if result["parsing_error"]:
            logger.error(f'parsing error: {result["parsing_error"]}')
            return {}

        analysis_report: VulnReport = result["parsed"]
        
        return analysis_report, llm.name
        

    async def generate(self, context: VulnerabilityResearchContext):
        context.state_info.node.append(self.NODES.generate)

        if context.reports:
            docs: list[Document] = []

            for report in context.reports:
                console.info(
                    f"[+] Found {report.cve_details.cve} security flaw in {report.artifact.primary_file.name}"
                )
                # res = VectorStore.reports.get(
                #     where={'$and': [{'cve': report.cve_details.cve},
                #                     {'file': report.artifact.primary_file.name}]},
                # )
                #
                # if not res.get('ids'):
                docs.append(
                    Document(
                        page_content=report.content,
                        metadata={
                            "cve": report.cve_details.cve,
                            "kb": report.artifact.primary_file.kb,
                            "file": report.artifact.primary_file.name,
                            "patch_store_uid": report.artifact.primary_file.uid,
                            "confidence": report.confidence,
                            "change_count": len(report.artifact.changed),
                            "date": time.time(),
                            "model_name": report.model,
                        },
                    )
                )

            if docs:
                try:
                    await VectorStore.reports.aadd_documents(
                        documents=docs,
                        ids=[str(uuid.uuid4()) for i in range(len(docs))],
                    )
                    console.info(
                        f"[+] {context.cve_details.cve}: {len(docs)} reports generated "
                        f"and cached successfully"
                    )
                except BaseException as e:
                    logger.error(f"Failed add to reports vector store: {e}")

        return {"reports": context.reports or []}
