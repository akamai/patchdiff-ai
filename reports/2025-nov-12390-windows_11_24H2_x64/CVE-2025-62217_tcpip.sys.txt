{'change_count': 43, 'patch_store_uid': '600ccb23-b28b-4f6d-b005-d53a5d35cf13', 'cve': 'CVE-2025-62217', 'file': 'tcpip.sys', 'kb': 'KB5068861', 'confidence': 0.1, 'date': 1763410055.1874998}
--------------------------------------------------------------------
Unknown-CVE Report
--------------------------------------------------------------------

Component
--------------------------------------------------------------------
Windows Ancillary Function Driver for WinSock (AFD.SYS) â€“ ALE   
(Access-control List Enforcement) cache handling routine            
AleEdgeIFsFlushCache().                                             

Vulnerability Class
--------------------------------------------------------------------
CWE-362: Concurrent execution using shared resource with improper  
synchronisation (race condition).

Detailed Root Cause Analysis
--------------------------------------------------------------------
The endpoint structure that represents a user socket contains a     
small cache of "edge" interface (IF) entries:                       
  +0x130 : spin-lock (embedded)                                     
  +0x130+0xB0 (0x304) : list head of cached IF records              
  +0x140+0x04 (0x324) : number of cached IF records                 
  +0x150+0x08 (0x328) : state flags (bit0 = cache valid)            
  +0x150+0x10 (0x336) : cached interface index.                     
                                                                  
AleEdgeIFsFlushCache() is invoked whenever the interface index      
attached to the socket changes.  The routine acquires the          
endpoint spin-lock, compares the caller-supplied index (a2) with    
the one stored at offset +0x336 and, when different, clears the     
old cache and drops the list pointer at +0x304 for later cleanup   
(by calling AleEdgeIFsCleanup()).                                   
                                                                  
In the vulnerable build the routine updates the structure members   
in the following (non-atomic) order under the lock:                 
  1. Writes the new interface index to +0x336                       
  2. NULLs the list head at +0x304                                  
  3. Clears the status bytes at +0x320 / +0x321                     
  4. Finally zeroes the element counter at +0x324.                  
                                                                  
Because the counter (+0x324) is written *after* the list head is     
reset, another thread that enters the code path immediately after   
step 2 will observe a NULL list pointer while the counter is still  
non-zero.  That thread will conclude that a cache already exists    
(size > 0) but that the list head is unexpectedly NULL, an          
invariant violation.  Subsequent code that iterates the list        
through the stale counter will therefore dereference a NULL / freed 
address, leading to pool corruption or an attacker-controlled       
use-after-free.  This enables local privilege escalation.           

Patch 27114 changes the write-order: the counter at +0x324 is now   
reset *first*, before any other field is modified, guaranteeing     
that a reader can never observe the inconsistent (counter!=0 &&     
list==NULL) state.  The function has also been converted from void  
to __int64 and now returns the value produced by AleEdgeIFsCleanup  
so that the caller can propagate any error code, but that change is 
cosmetic with regard to the race.                                   

Vulnerability Code Snippets
--------------------------------------------------------------------
```c
// vulnerable order (before)
if (a2 != *(DWORD *)(a1 + 336)) {
    *(DWORD *)(a1 + 336) = a2;      // 1 new index
    *(QWORD *)(a1 + 304) = 0;       // 2 NULL list head
    *(BYTE  *)(a1 + 320) = 0;
    *(BYTE  *)(a1 + 321) = 0;
    *(DWORD *)(a1 + 324) = 0;       // 4 counter reset (too late)
}

// fixed order (after)
if (a2 != *(DWORD *)(a1 + 336)) {
    *(DWORD *)(a1 + 324) = 0;       // 1 counter first
    *(DWORD *)(a1 + 336) = a2;
    *(QWORD *)(a1 + 304) = 0;
    *(BYTE  *)(a1 + 320) = 0;
    *(BYTE  *)(a1 + 321) = 0;
    v3 = *(QWORD *)(a1 + 312);
    *(QWORD *)(a1 + 312) = 0;
}
```

Trigger Flow (Top-Down)
--------------------------------------------------------------------
1. Thread A (local attacker) issues socket operation that causes an 
   interface index change and enters AleEdgeIFsFlushCache().        
2. Thread A takes the endpoint spin-lock, performs step 1 & 2 of    
   the vulnerable sequence, then gets pre-empted.                   
3. Thread B (victim or attacker-controlled) now enters               
   AleEdgeIFsFlushCache() or any other routine that looks at the    
   cache.  It sees counter>0 and list==NULL, dereferences invalid   
   memory and corrupts kernel pool.                                 
4. The corrupted pool allows the attacker to escalate privileges.   

Attack Vector
--------------------------------------------------------------------
Local, authenticated attacker running crafted Winsock operations on 
multiple threads / cores to race the ALE endpoint cache flush.      
No elevated privileges or special capabilities required.            

Patch Description
--------------------------------------------------------------------
Microsoft moved the write to the cache counter (+0x324) ahead of     
all other field updates, restoring atomicity of the cache state.    
The return type was also switched to __int64 so that the status      
returned by AleEdgeIFsCleanup() can be propagated, but that has no  
impact on exploitation.                                             

Security Impact
--------------------------------------------------------------------
Prior to the patch a local attacker could reliably hit a            
use-after-free / null-dereference window inside the network stack,  
leading to elevation of privilege (ring-0 execution) or, at a       
minimum, a denial of service (bugcheck).                            

Fix Effectiveness
--------------------------------------------------------------------
The reorder ensures that at no time can another thread observe an   
invalid combination of list pointer and element count.  As both     
values are written while holding the same spin-lock, the race       
condition is completely removed.  No residual paths that use the    
old ordering remain, so the fix is judged effective.                
