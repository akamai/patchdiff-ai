{'patch_store_uid': '39f7108f-f9c3-429c-aab6-90422b62deb3', 'date': 1763406205.2061565, 'cve': 'CVE-2025-50152', 'kb': 'KB5066835', 'file': 'securekernel.exe', 'confidence': 0.14, 'change_count': 8}
--------------------------------------------------------------------
CVE-2025-50152 Report
--------------------------------------------------------------------

Component
--------------------------------------------------------------------
Windows Secure Kernel – extended processor-state (XSTATE) helper
routines located in securekernel.exe (functions such as
RtlGetExtendedContextLength2, RtlInitializeExtendedContext2,
RtlpCopyXStateChunk and RtlpCopyLegacyContextArm64).

Vulnerability Class
--------------------------------------------------------------------
Out-of-bounds read / size-calculation error (CWE-125).

Detailed Root Cause Analysis
--------------------------------------------------------------------
User-mode callers supply a feature bitmap that identifies which x86
/ ARM64 XSTATE blocks they wish to operate on (e.g. via
NtContinue/NtGetContext/NtSetContext or SKMM image validation code).

Before the patch the secure-kernel helper RtlpGetLegacyContextLength
handled only four legacy bits (0x10000, 0x100000, 0x200000,
0x400000).  Any other bit positions (2-63) were silently ignored and
the returned size (v15) reflected only the legacy blocks.  The size
is later trusted by
 • RtlGetExtendedContextLength2 – to allocate a caller-controlled
   buffer; and
 • RtlpCopyXStateChunk / RtlpCopyLegacyContextArm64 – to copy
   individual XSTATE sub-chunks in a 0-to-0x3F loop.

Because the copy loop iterates over all 64 feature bits while the
allocated buffer may only cover the four legacy regions, the kernel
reads (and in several cases writes) past the end of the caller’s
buffer.  The over-read occurs inside secure kernel context and leaks
adjacent kernel memory to userland; in some code paths it can also be
turned into an out-of-bounds write, leading to local elevation of
privilege.

Patch analysis:
1.  The legacy helper was replaced with RtlpGetEntireXStateAreaLength2
    which iterates from bit 2 to 0x3F, consults the per-feature size
    table at KPCR+544 and applies 64-byte alignment when required.
2.  RtlGetExtendedContextLength2 was rewritten to compute the size
    itself when only legacy bits are requested and to call the new
    helper for any extended bit combination.  Additional masking is
    performed against the architectural allow mask held in
    KUSER_SHARED_DATA (0xFFFFF780…3D8/5F0/5F8).
3.  RtlInitializeExtendedContext2 mirrors the new size logic and now
    sanitises the feature bitmap before performing any copy or zero
    operation.
4.  RtlpCopyXStateChunk adds stricter bounds checks (v6..v10) and uses
    the calculated lengths instead of hard-coded values.
5.  RtlpCopyLegacyContextArm64 now tests for “feature set”
    combinations (0x40 and 0x80 blocks) and copies only the validated
    portions.

Missing validation of non-legacy bits therefore constituted the root
cause; the patch changes every size calculation site so that every
feature bit contributes to the returned length, removing the original
size mismatch.

Vulnerability Code Snippets
--------------------------------------------------------------------
Before (RtlpGetLegacyContextLength):
```c
v3 = 0;              // ignored bits → no length added
result = 0;
if (a1 & 0x10000) { v3 = 716; result = 4; }
// …only four cases handled …
return result;       // may be 0 even if other bits are set
```
After (RtlpGetEntireXStateAreaLength2):
```c
v3 = 2;
result = 576;                    // base area
v4 = *(_QWORD *)(a2 + 544);      // per-feature align mask
v6 = (DWORD *)(a2 + 564);        // per-feature size table
while (v3 < 0x40) {
    if (((1ull << v3) & a1)) {
        if (((1ull << v3) & v4)) result = (result + 63) & ~63;
        result += *v6;           // add correct sub-size
    }
    ++v3; ++v6;
}
```

Trigger Flow (Top-Down)
--------------------------------------------------------------------
1. User supplies crafted CONTEXT/XSTATE bitmap to a syscall that ends
   up in secure kernel.
2. RtlGetExtendedContextLength2 → RtlpGetLegacyContextLength (old)
   returns undersized buffer length.
3. Caller allocates buffer of that size and passes it back to kernel
   (SetContext / NtContinue / SKMM image validation).
4. RtlpCopyXStateChunk iterates 0-0x3F, copying data into the user
   buffer.  When it reaches an extended bit the calculated offset
   (v9/v7) exceeds the buffer → OOB read/write in kernel.

Attack Vector
--------------------------------------------------------------------
Local, authenticated attacker invokes user-accessible APIs that move
thread/process CONTEXT records (e.g. NtSetContextThread) with a
malicious FeatureMask containing extended bits but provides only a
minimal legacy-sized buffer.  The secure kernel will touch memory
beyond the caller-supplied buffer, leaking kernel data and potentially
corrupting adjacent kernel structures, resulting in privilege
escalation.

Patch Description
--------------------------------------------------------------------
• Introduced RtlpGetEntireXStateAreaLength2 which enumerates every
  feature bit and aligns/aggregates its real size using tables in the
  KPCR.
• Re-implemented RtlGetExtendedContextLength2 and
  RtlInitializeExtendedContext2 with the new length logic and with
  additional architectural mask checks.
• Hardened RtlpCopyXStateChunk and RtlpCopyLegacyContextArm64 to use
  calculated per-feature lengths and to bail out if the destination
  buffer is smaller than required.
• Added bitmap sanitation paths for secure-image validation code in
  SkmmValidateSecureImagePages.

Security Impact
--------------------------------------------------------------------
Prior to the fix an unprivileged local attacker could trigger kernel
memory disclosure and, in some paths, overwrite memory controlled by
privileged components, thereby gaining SYSTEM-level execution.  The
issue is classified as an elevation-of-privilege vulnerability.

Fix Effectiveness
--------------------------------------------------------------------
The patched code now:
1. Computes the exact XSTATE size for every possible bit.
2. Aligns each block when the CPU requires it.
3. Validates the requested bitmap against the per-CPU allow mask.
4. Performs strict bounds checks before every memmove/memcpy.

These changes close the size-mismatch window; no obvious path remains
for a crafted bitmap to obtain a smaller buffer than the subsequent
copy routines expect.  The fix therefore appears effective.
